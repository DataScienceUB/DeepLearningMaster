{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13. Reinforcement learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tul5I9VqXMC",
        "colab_type": "code",
        "colab": {},
        "outputId": "16c2f4b6-95e9-4ad7-d817-f43f5ed627e7"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REDbsHwXqXMI",
        "colab_type": "code",
        "colab": {},
        "outputId": "8708d355-1fb7-44ae-b890-e7a0f5118fb4"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/74/ecd13431bcc456ed390b44c8a6e917c1820365cbebcb6a8974d1cd045ab4/pip-10.0.1-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K    100% |################################| 1.3MB 817kB/s eta 0:00:01    54% |#################               | 716kB 5.1MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 9.0.1\n",
            "    Uninstalling pip-9.0.1:\n",
            "      Successfully uninstalled pip-9.0.1\n",
            "Successfully installed pip-10.0.1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.5/dist-packages (0.9.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.5/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from gym) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.5/dist-packages (from gym) (1.14.0)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests>=2.0->gym) (2018.1.18)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.5/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmhtoqY3qXMK",
        "colab_type": "text"
      },
      "source": [
        "#  Reinforcement Learning\n",
        "\n",
        "> *Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take **actions in an environment** so as to maximize some notion of **cumulative reward**. (Source: Wikipedia)*\n",
        "\n",
        "Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through it’s own interactions with an environment. \n",
        "\n",
        "By combining the complex representations that deep neural networks can learn with the goal-driven learning of an RL agent, computers have accomplished some amazing feats, like beating humans at over a dozen Atari games, and defeating the Go world champion.\n",
        "\n",
        "RL algorithms must enable the agent to learn the correct pairings itself through the use of *observations*, *rewards*, and *actions*.\n",
        "\n",
        "Typical aspects of a task that make it an RL problem are the following:\n",
        "1. Different actions yield different rewards. For example, when looking for treasure in a maze, going left may lead to the treasure, whereas going right may lead to a pit of snakes.\n",
        "2. Rewards are delayed over time. This just means that even if going left in the above example is the right things to do, we may not know it till later in the maze.\n",
        "3. Reward for an action is conditional on the state of the environment. Continuing the maze example, going left may be ideal at a certain fork in the path, but not at others.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCTsY2EjqXMK",
        "colab_type": "text"
      },
      "source": [
        "## Multi-armed bandit\n",
        "\n",
        "The simplest reinforcement learning problem is the multi-armed bandit. Essentially, there are $n$-many slot machines, each with a different fixed payout probability. \n",
        "\n",
        "The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. \n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/slot.jpg?raw=1\" alt=\"\" style=\"width: 400px;\"/>\n",
        "\n",
        "This question has been the subject of active research since the 1950s, and many variations have been studied.\n",
        "\n",
        "#### A-B Testing\n",
        "\n",
        "Traditional A-B testing can be thought of as a special case of the multi-armed bandit problem, in which we choose to pursue a strategy of pure exploration in the initial testing phase, followed by a period of pure exploitation in which we choose the most valuable “arm” 100% of the time.\n",
        "\n",
        "If the exploitation phase can be assumed to be much longer than the exploration phase, this approach is usually reasonable, as the wasted resources during the exploration are insignificant relative to the total rewards. However, in cases where the cost of the exploration phase is non-negligible, or in cases in which arm values are changing dynamically on short enough timescales that it becomes impractical to repeatedly perform new A-B tests, alternative approaches are needed.\n",
        "\n",
        "The n-armed bandit is a nice starting place because we don’t have to worry about aspects #2 and #3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbkAevR3qXML",
        "colab_type": "text"
      },
      "source": [
        "### Epsilon-greedy\n",
        "\n",
        "The most straightforward algorithm for continuously balancing exploration with exploitation is called “epsilon-greedy”. \n",
        "\n",
        "Here, we pull a randomly chosen arm a fraction $\\epsilon$ of the time. The other $1-\\epsilon$ of the time, we pull the arm which we estimate to be the most profitable. As each arm is pulled and rewards are received, our estimates of arm values are updated. \n",
        "\n",
        "This method can be thought of a a continuous testing setup, where we devote a fraction $\\epsilon$ of our resources to testing.\n",
        "\n",
        "The following python code implements a simple 10-Armed Bandit using the epsilon-greedy algorithm. \n",
        "\n",
        "The payout rate of the arms are normally distributed with $\\mu = 0$ and $\\sigma = 1$. Gaussian noise is also added to the rewards, also with $\\mu = 0$ and $\\sigma = 1$. (See [Sutton and Barto](http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf) book, section 2.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue1k_ifjqXMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "249981cb-6308-4edd-e513-e472d3337952"
      },
      "source": [
        "import numpy as np \n",
        "import seaborn \n",
        "from tqdm import tqdm\n",
        "\n",
        "class Bandit: \n",
        "    def __init__(self): \n",
        "        self.arm_values = np.random.normal(0,1,10) #arm values\n",
        "        self.K = np.zeros(10)                      #action counter\n",
        "        self.est_values = np.zeros(10)             #estimated values\n",
        "\n",
        "    def get_reward(self,action):                   #get noisy reward from machine \"action\"\n",
        "        noise = np.random.normal(0,1) \n",
        "        reward = self.arm_values[action] + noise \n",
        "        return reward \n",
        "\n",
        "    def choose_eps_greedy(self,epsilon):           #exploitation or exploration decision\n",
        "        rand_num = np.random.random() \n",
        "        if epsilon>rand_num: \n",
        "            return np.random.randint(10)           #exploration\n",
        "        else: \n",
        "            return np.argmax(self.est_values)      #exploitation\n",
        "\n",
        "    def update_est(self,action,reward):            #estimation of the value of an action\n",
        "        self.K[action] += 1 \n",
        "        alpha = 1./self.K[action] \n",
        "        self.est_values[action] += alpha * (reward - self.est_values[action]) \n",
        "        \n",
        "a= Bandit()\n",
        "print(\"The arm vales of the 10 machines are: \\n\",a.arm_values)\n",
        "print(\"The reward if we play first machine is: \\n\",a.get_reward(0))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The arm vales of the 10 machines are: \n",
            " [ 1.1796864   0.40726757  0.31826565  0.75944305  0.44926722  1.04328852\n",
            "  0.31688177 -0.20897799 -1.0835095   0.67392407]\n",
            "The reward if we play first machine is: \n",
            " 2.366974080810383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDkI0LSEqXMO",
        "colab_type": "text"
      },
      "source": [
        "How are we estimating the value of an action?\n",
        "\n",
        "If by the $t$-th time step action $a$ has been chosen $K_a$ times prior to $t$, yielding rewards \n",
        "$R_1, R_2, . . . , R_{Ka}$, then its value is estimated to be:\n",
        "\n",
        "$$\n",
        "Q_t(a) = \\frac{R_1 + R_2 + . . .+ R_{Ka}}{K_a}\n",
        "$$\n",
        "\n",
        "A problem with this straightforward implementation is that its memory and computational requirements grow over time without\n",
        "bound (we have to maintain, for each action $a$, a record of all the rewards that have followed the\n",
        "selection of that action), but we can derive an incremental formula for computing averages with small, constant computation\n",
        "required to process each new reward.\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "Q_{k+1} & = & \\frac{1}{k+1} \\sum_{i=1}^{k+1} R_i \\\\\n",
        "  & = & \\frac{1}{k+1} \\left( R_{k+1} + \\sum_{i=1}^k R_i\n",
        "\\right) \\\\\n",
        "  & = & \\frac{1}{k+1} \\left( R_{k+1} + kQ_k + Q_k - Q_k\n",
        "\\right) \\\\\n",
        "  & = & \\frac{1}{k+1} \\left(R_{k+1} + (k+1) Q_k - Q_k\n",
        "\\right) \\\\\n",
        "  & = & Q_k + \\frac{1}{k+1} \\left( R_{k+1} - Q_k \\right)\n",
        "\\end{eqnarray*}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-EfFQJtqXMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(bandit,Npulls,epsilon):\n",
        "    history = [] \n",
        "    for i in range(Npulls): \n",
        "        action = bandit.choose_eps_greedy(epsilon)\n",
        "        R = bandit.get_reward(action) \n",
        "        bandit.update_est(action,R) \n",
        "        history.append(R) \n",
        "    return np.array(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IObBsHLcqXMQ",
        "colab_type": "text"
      },
      "source": [
        "Let's make three different experiments: $\\epsilon = 0$, $\\epsilon = 0.1$ and $\\epsilon = 0.01$. Data will be averages over 2000 tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le9GmMEqqXMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "2c8e5e45-dd45-4915-9e28-1e046a4ec4b2"
      },
      "source": [
        "Nexp = 2000\n",
        "Npulls = 5000 \n",
        "avg_outcome_eps0p0 = np.zeros(Npulls) \n",
        "avg_outcome_eps0p01 = np.zeros(Npulls) \n",
        "avg_outcome_eps0p1 = np.zeros(Npulls) \n",
        "\n",
        "for i in tqdm(range(Nexp)): \n",
        "    bandit = Bandit() \n",
        "    avg_outcome_eps0p0 += experiment(bandit,Npulls,0.0) \n",
        "    bandit = Bandit() \n",
        "    avg_outcome_eps0p01 += experiment(bandit,Npulls,0.01) \n",
        "    bandit = Bandit() \n",
        "    avg_outcome_eps0p1 += experiment(bandit,Npulls,0.1) \n",
        "\n",
        "avg_outcome_eps0p0 /= np.float(Nexp) \n",
        "avg_outcome_eps0p01 /= np.float(Nexp) \n",
        "avg_outcome_eps0p1 /= np.float(Nexp) \n",
        "\n",
        "# plot results \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "plt.plot(avg_outcome_eps0p0,label=\"eps = 0.0\", alpha=0.5) \n",
        "plt.plot(avg_outcome_eps0p01,label=\"eps = 0.01\", alpha=0.5) \n",
        "plt.plot(avg_outcome_eps0p1,label=\"eps = 0.1\", alpha=0.5) \n",
        "plt.ylim(0,2.2) \n",
        "plt.legend() \n",
        "plt.gcf().set_size_inches((8,3))\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [03:35<00:00,  9.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAADFCAYAAABuHjrdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47bk4a_-qXMS",
        "colab_type": "text"
      },
      "source": [
        "Although ε-greedy action selection is an effective and popular means of balancing exploration and exploitation in reinforcement learning, one drawback is that when it explores **it chooses equally among all actions**. \n",
        "\n",
        "This means that it is as likely to choose the worst-appearing action as it is to choose the next-to-best action. In tasks where the worst actions are very bad, this may\n",
        "be unsatisfactory. The obvious solution is to vary the action probabilities as a graded function of estimated value.\n",
        "\n",
        "The greedy action will be given the highest selection probability, but all the others will be ranked and weighted according to\n",
        "their value estimates. To this end we can use a softmax action selection rule (with a temperature parameter $\\tau$):\n",
        "\n",
        "$$\n",
        "\\frac{ e^{Q_t(a)/ \\tau}}{\\sum_{i=1}^n e^{Q_t(i)/ \\tau}}\n",
        "$$\n",
        "\n",
        "For high temperatures ( $\\tau \\to \\infty $ ), all actions have nearly the same probability. For a low temperature ( $ \\tau \\to 0^{+} $), the probability of the action with the highest expected reward tends to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNhWHWZSqXMT",
        "colab_type": "text"
      },
      "source": [
        "### Contextual Bandit\n",
        "\n",
        "In the bandit problem described above, it is assumed that **nothing is known about each arm other than what we have learned from prior pulls**.\n",
        "\n",
        "We can relax this assumption and assume that for each arm there is a d-dimensional “context” vector. For example, if each arm represents a digital ad, the features in these vectors may correspond to things like banner size, web browser type, font color, etc. We can now model the value of each arm using these context vectors as well as past rewards in order to inform our choice of which arm to pull. This scenario is known as the contextual bandit problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ayQwNIqXMT",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning\n",
        "\n",
        "Unlike other methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts **to learn the value of being in a given state, and taking a specific action there**. \n",
        "\n",
        "The agent is in a state $s$ and has to choose one action $a$, upon which it receives a reward $r$ and come to a new state $s’$. The way the agent chooses actions is called **policy**.\n",
        "\n",
        "Let’s define a function $Q(s, a)$ such that for given state $s$ and action $a$ it returns an estimate of a total reward we would achieve starting at this state, taking the action and then following some policy. Under certain conditions, there certainly exist policies that are optimal, meaning that they always select an action which is the best in the context. Let’s call the $Q$ function for these optimal policies $Q^*$.\n",
        "\n",
        "If we knew the true $Q^*$ function, the solution would be straightforward. We would just apply a greedy policy to it. That means that in each state $s$, we would just choose an action $a$ that maximizes the function $Q^*$, $argmax_a Q^*(s, a)$. Knowing this, our problem reduces to find a good estimate of the $Q^*$ function and apply the greedy policy to it.\n",
        "\n",
        "Let’s write a formula for this function in a symbolic way. It is a sum of rewards we achieve after each action, but we will discount every member with γ:\n",
        "\n",
        " $$ Q^*(s, a) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\gamma^3 r_3 + ... $$\n",
        "\n",
        "$\\gamma$ is called a discount factor and when set it to $\\gamma < 1$ , it makes sure that the sum in the formula is finite. Value of each member exponentially diminish as they are more and more in the future and become zero in the limit. The $\\gamma$ therefore controls how much the function $Q$ in state $s$ depends on the future and so it can be thought of as how much ahead the agent sees. \n",
        "\n",
        "Typically we set it to a value close, but lesser to one. The actions are chosen according to the greedy policy, maximizing the $Q^*$ function.\n",
        "\n",
        "When we look again at the formula, we see that we can write it in a recursive form:\n",
        "\n",
        " $$Q^*(s, a) = r_0 + \\gamma (r_1 + \\gamma r_2 + \\gamma^2 r_3 + ...) = r_0 + \\gamma max_a Q^*(s', a)$$\n",
        " \n",
        "We just derived a so called **Bellman equation**.\n",
        " \n",
        "One of the possible strategies to solve the Bellman equation is by applying the **Q-learning** algorithm:\n",
        "\n",
        "```\n",
        "For each state-action pair (s, a), initialize the table entry Q(s,a) to zero\n",
        "Observe the current state s\n",
        "Do forever:\n",
        "- Select an action a from s and execute it \n",
        "- Receive immediate reward r\n",
        "- Observe the new state s'\n",
        "- Update the table entry for Q\n",
        "- s=s'\n",
        "```\n",
        "#### Action selection\n",
        "\n",
        "We could apply different strategies for action selection:\n",
        "+ **Random approach**. Only in circumstances where a random policy is optimal would this approach be ideal.\n",
        "+ **$\\epsilon$- greedy approach**:  A simple combination of the greedy and random approaches yields one of the most used exploration strategies. At the start of the training process the $\\epsilon$ value is often initialized to a large probability, to encourage exploration in the face of knowing little about the environment. The value is then annealed down to a small constant (often 0.1), as the agent is assumed to learn most of what it needs about the environment. Despite the prevalence of usage that it enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.\n",
        "+ **Boltzmann Approach**. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action.  In practice we utilize an additional temperature parameter ($\\tau$) which is annealed over time. While this measure can be a useful proxy, it is not exactly what would best aid exploration. What we really want to understand is the agent’s uncertainty about the value of different actions.\n",
        "+ **Bayesian Approaches**. What if an agent could exploit its own uncertainty about its actions? This is exactly the ability that a class of neural network models referred to as Bayesian Neural Networks (BNNs) provide. Unlike traditional neural network which act deterministically, BNNs act probabilistically. This means that instead of having a single set of fixed weights, a BNN maintains a probability distribution over possible weights. In a reinforcement learning setting, the distribution over weight values allows us to obtain distributions over actions as well. The variance of this distribution provides us an estimate of the agent’s uncertainty about each action. In order to get true uncertainty estimates, multiple samples are required, thus increasing computational complexity.\n",
        "\n",
        "\n",
        "#### Table updating\n",
        "\n",
        "The table entry for $Q$ is updated by using this formula:\n",
        "\n",
        "$$\n",
        "Q(s,a) = Q(s,a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a) ]\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "+ $0<\\alpha<1$ is the learning rate. Setting it to 0 means that the Q-values are never updated, hence nothing is learned. Setting a high value such as 0.9 means that learning can occur quickly.\n",
        "+ $0<\\gamma<1$ is the discount factor. This models the fact that future rewards are worth less than immediate rewards. \n",
        "\n",
        "See http://www.scholarpedia.org/article/Temporal_difference_learning for a short description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MWg2N2hqXMU",
        "colab_type": "text"
      },
      "source": [
        "### Solving the FrozenLake problem\n",
        "\n",
        "We are going to to solve the FrozenLake environment from the OpenAI gym. \n",
        "\n",
        "OpenAI gym (https://gym.openai.com/) provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. \n",
        "\n",
        "> **FrozenLake-v0**\n",
        "\n",
        "> The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "> The surface is described using a grid like the following:\n",
        "\n",
        ">``SFFF       (S: starting point, safe)``\n",
        "\n",
        ">``FHFH       (F: frozen surface, safe)``\n",
        "\n",
        ">``FFFH       (H: hole, fall to your doom)``\n",
        "\n",
        ">``HFFG       (G: goal, where the frisbee is located)``\n",
        "\n",
        "> The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either ``up, down, left``, or ``right``. \n",
        "\n",
        "The catch is that there is a wind which occasionally blows the agent onto a space they didn’t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KicUducwqXMU",
        "colab_type": "text"
      },
      "source": [
        "In it’s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. \n",
        "\n",
        "In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a $16 \\times 4$ table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqh9n7XyqXMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F-wv_ViqXMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "1ac26626-c745-43e4-a587-4321e829def5"
      },
      "source": [
        "#Initialize table with all zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "\n",
        "# Set learning parameters\n",
        "lr = .9\n",
        "gamma = 0.95\n",
        "num_episodes = 10000\n",
        "\n",
        "#create lists to contain total rewards and steps per episode\n",
        "rList = []\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s = env.reset()\n",
        "    rAll = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "    \n",
        "    #The Q-Table learning algorithm\n",
        "    while j < 999999:\n",
        "        j+=1\n",
        "        #Choose an action by greedily (with noise) picking from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        #Get new state and reward from environment\n",
        "        s1,r,d,_ = env.step(a)\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,a] = Q[s,a] + lr*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        if d == True:\n",
        "            break\n",
        "    rList.append(rAll)\n",
        "    \n",
        "print(\"Score over time: \" +  str(sum(rList[-100:])/100))\n",
        "print(\"Final Q-Table Values\")\n",
        "print(Q)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.8\n",
            "Final Q-Table Values\n",
            "[[1.70216414e-01 9.91289589e-04 8.07651609e-04 9.94384986e-04]\n",
            " [3.11849273e-05 2.41157068e-04 2.23941046e-04 3.10609046e-01]\n",
            " [2.52090813e-01 1.00321309e-04 1.47766959e-04 6.91033227e-04]\n",
            " [6.06716105e-04 6.52081250e-04 7.59932127e-05 6.45606522e-04]\n",
            " [6.93535446e-02 1.41301294e-04 6.20534292e-04 2.38752473e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.77946039e-07 8.62823977e-08 1.47888298e-01 4.44273898e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.79765630e-05 1.62084109e-04 5.82167070e-06 5.55430222e-02]\n",
            " [2.78345654e-05 6.73250716e-01 6.70158380e-05 1.85569486e-05]\n",
            " [9.11405139e-01 8.56105056e-06 1.65664270e-06 2.37234780e-05]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 5.98543694e-01 0.00000000e+00]\n",
            " [0.00000000e+00 9.57241931e-01 0.00000000e+00 5.06933574e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qowXSM2NqXMb",
        "colab_type": "text"
      },
      "source": [
        "FrozenLake-v0 is considered \"solved\" when the agent obtains an average reward of at least 0.78 over 100 consecutive episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hclfnwZyqXMf",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning with Neural Networks\n",
        "\n",
        "Now, you may be thinking: tables are great, but they don’t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don’t work. \n",
        "\n",
        "We instead need some way to take a description of our state, and produce $Q$-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to $Q$-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rstH67xgqXMg",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/pong.jpg?raw=1\" alt=\"\" style=\"width: 300px;\"/>\n",
        "\n",
        "\n",
        "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 $Q$-values, one for each action. \n",
        "\n",
        "Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. \n",
        "\n",
        "The method of updating is a little different as well. \n",
        "\n",
        "Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted $Q$-values, and the “target” value is computed and the gradients passed through the network. \n",
        "\n",
        "In this case, our $Q_{target}$ for the chosen action is the equivalent to the $Q$-value computed in equation above ($\n",
        "Q(s,a) + \\alpha [r + \\gamma max_{a'} Q(s',a') - Q(s,a) ]\n",
        "$).\n",
        "\n",
        "$$\n",
        "Loss = \\sum (Q_{target} - Q_{predicted})^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNdiYcYyqXMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jac8lo1jqXMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "#These lines establish the feed-forward part of the network used to choose actions\n",
        "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
        "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
        "Qout = tf.matmul(inputs1,W)\n",
        "predict = tf.argmax(Qout,1)\n",
        "\n",
        "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
        "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
        "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
        "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "updateModel = trainer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeuHz50PqXMk",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc30b9b8-b4bf-43ba-bf2d-f4e2c69edca4"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Set learning parameters\n",
        "y = .99\n",
        "e = 0.1\n",
        "num_episodes = 2000\n",
        "\n",
        "#create lists to contain total rewards and steps per episode\n",
        "jList = []\n",
        "rList = []\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(num_episodes):\n",
        "        #Reset environment and get first new observation\n",
        "        s = env.reset()\n",
        "        rAll = 0\n",
        "        d = False\n",
        "        j = 0\n",
        "        #The Q-Network\n",
        "        while j < 99:\n",
        "            j+=1\n",
        "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
        "            a,allQ = sess.run([predict,Qout],\n",
        "                              feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
        "            if np.random.rand(1) < e:\n",
        "                a[0] = env.action_space.sample()\n",
        "            #Get new state and reward from environment\n",
        "            s1,r,d,_ = env.step(a[0])\n",
        "            #Obtain the Q' values by feeding the new state through our network\n",
        "            Q1 = sess.run(Qout,\n",
        "                          feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
        "            #Obtain maxQ' and set our target value for chosen action.\n",
        "            maxQ1 = np.max(Q1)\n",
        "            targetQ = allQ\n",
        "            targetQ[0,a[0]] = r + y*maxQ1\n",
        "            #Train our network using target and predicted Q values\n",
        "            _,W1 = sess.run([updateModel,W],\n",
        "                            feed_dict={inputs1:np.identity(16)[s:s+1],\n",
        "                                       nextQ:targetQ})\n",
        "            rAll += r\n",
        "            s = s1\n",
        "            if d == True:\n",
        "                #Reduce chance of random action as we train the model.\n",
        "                e = 1./((i/50) + 10)\n",
        "                break\n",
        "        jList.append(j)\n",
        "        rList.append(rAll)\n",
        "print(\"Percent of succesful episodes: \" + str(sum(rList[-100:])/100) + \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of succesful episodes: 0.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh7im78ZqXMm",
        "colab_type": "text"
      },
      "source": [
        "We can see that the network beings to consistly reach the goal around the 600 episode mark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwkXMdciqXMm",
        "colab_type": "code",
        "colab": {},
        "outputId": "0dcbcb96-9f90-4bc0-8cc0-0acefc427cea"
      },
      "source": [
        "plt.plot(rList)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f33bc09c630>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f33bc0c7400>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi8BStR5qXMs",
        "colab_type": "text"
      },
      "source": [
        "It also begins to progress through the environment for longer than chance around the 600 mark as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x01U6BLjqXMt",
        "colab_type": "code",
        "colab": {},
        "outputId": "b76f8c41-95c0-41b8-d64d-85135f8d3fd7"
      },
      "source": [
        "plt.plot(jList)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f33bc05d2e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f33bc0875f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYI8q4jXqXMw",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q-networks\n",
        "\n",
        "While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep $Q$-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
        "+ Going from a single-layer network to a multi-layer convolutional network.\n",
        "+ Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
        "+ Utilizing a second “target” network, which we will use to compute target $Q$-values during our updates.\n",
        "\n",
        "<img src=\"https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/deepq1.png?raw=1\" alt=\"\" style=\"width: 800px;\"/>\n",
        "\n",
        "\n",
        "See https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I75sIlTpqXMx",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Layers\n",
        "\n",
        "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network.\n",
        "\n",
        "### Experience Replay\n",
        "\n",
        "The second major addition to make DQNs work is Experience Replay. \n",
        "\n",
        "The problem with online learning is that the *samples arrive in order* they are experienced and as such are highly correlated. Because of this, our network will most likely overfit and fail to generalize properly.\n",
        "\n",
        "The key idea of **experience replay** is that we store these transitions in our memory and during each learning step, sample a random batch and perform a gradient descend on it. \n",
        "\n",
        "The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. \n",
        "\n",
        "### Separate Target Network\n",
        "\n",
        "This second network is used to generate the $Q$-target values that will be used to compute the loss for every action during training. \n",
        "\n",
        "The issue is that at every step of training, the $Q$-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated $Q$-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary $Q$-networks values. In this way training can proceed in a more stable manner.\n",
        "\n",
        "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dngNSL5rqXMx",
        "colab_type": "text"
      },
      "source": [
        "While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4k93XA8qXMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.slim as slim\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc\n",
        "import os\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh44d7g-qXM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import itertools\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class gameOb():\n",
        "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
        "        self.x = coordinates[0]\n",
        "        self.y = coordinates[1]\n",
        "        self.size = size\n",
        "        self.intensity = intensity\n",
        "        self.channel = channel\n",
        "        self.reward = reward\n",
        "        self.name = name\n",
        "        \n",
        "class gameEnv():\n",
        "    def __init__(self,partial,size):\n",
        "        self.sizeX = size\n",
        "        self.sizeY = size\n",
        "        self.actions = 4\n",
        "        self.objects = []\n",
        "        self.partial = partial\n",
        "        a = self.reset()\n",
        "        plt.imshow(a,interpolation=\"nearest\")\n",
        "        \n",
        "        \n",
        "    def reset(self):\n",
        "        self.objects = []\n",
        "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
        "        self.objects.append(hero)\n",
        "        bug = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "        self.objects.append(bug)\n",
        "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
        "        self.objects.append(hole)\n",
        "        bug2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "        self.objects.append(bug2)\n",
        "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
        "        self.objects.append(hole2)\n",
        "        bug3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "        self.objects.append(bug3)\n",
        "        bug4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
        "        self.objects.append(bug4)\n",
        "        state = self.renderEnv()\n",
        "        self.state = state\n",
        "        return state\n",
        "\n",
        "    def moveChar(self,direction):\n",
        "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
        "        hero = self.objects[0]\n",
        "        heroX = hero.x\n",
        "        heroY = hero.y\n",
        "        penalize = 0.\n",
        "        if direction == 0 and hero.y >= 1:\n",
        "            hero.y -= 1\n",
        "        if direction == 1 and hero.y <= self.sizeY-2:\n",
        "            hero.y += 1\n",
        "        if direction == 2 and hero.x >= 1:\n",
        "            hero.x -= 1\n",
        "        if direction == 3 and hero.x <= self.sizeX-2:\n",
        "            hero.x += 1     \n",
        "        if hero.x == heroX and hero.y == heroY:\n",
        "            penalize = 0.0\n",
        "        self.objects[0] = hero\n",
        "        return penalize\n",
        "    \n",
        "    def newPosition(self):\n",
        "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
        "        points = []\n",
        "        for t in itertools.product(*iterables):\n",
        "            points.append(t)\n",
        "        currentPositions = []\n",
        "        for objectA in self.objects:\n",
        "            if (objectA.x,objectA.y) not in currentPositions:\n",
        "                currentPositions.append((objectA.x,objectA.y))\n",
        "        for pos in currentPositions:\n",
        "            points.remove(pos)\n",
        "        location = np.random.choice(range(len(points)),replace=False)\n",
        "        return points[location]\n",
        "\n",
        "    def checkGoal(self):\n",
        "        others = []\n",
        "        for obj in self.objects:\n",
        "            if obj.name == 'hero':\n",
        "                hero = obj\n",
        "            else:\n",
        "                others.append(obj)\n",
        "        ended = False\n",
        "        for other in others:\n",
        "            if hero.x == other.x and hero.y == other.y:\n",
        "                self.objects.remove(other)\n",
        "                if other.reward == 1:\n",
        "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
        "                else: \n",
        "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
        "                return other.reward,False\n",
        "        if ended == False:\n",
        "            return 0.0,False\n",
        "\n",
        "    def renderEnv(self):\n",
        "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
        "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
        "        a[1:-1,1:-1,:] = 0\n",
        "        hero = None\n",
        "        for item in self.objects:\n",
        "            a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
        "            if item.name == 'hero':\n",
        "                hero = item\n",
        "        if self.partial == True:\n",
        "            a = a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
        "        b = scipy.misc.imresize(a[:,:,0],[84,84,1],interp='nearest')\n",
        "        c = scipy.misc.imresize(a[:,:,1],[84,84,1],interp='nearest')\n",
        "        d = scipy.misc.imresize(a[:,:,2],[84,84,1],interp='nearest')\n",
        "        a = np.stack([b,c,d],axis=2)\n",
        "        return a\n",
        "\n",
        "    def step(self,action):\n",
        "        penalty = self.moveChar(action)\n",
        "        reward,done = self.checkGoal()\n",
        "        state = self.renderEnv()\n",
        "        if reward == None:\n",
        "            print(done)\n",
        "            print(reward)\n",
        "            print(penalty)\n",
        "            return state,(reward+penalty),done\n",
        "        else:\n",
        "            return state,(reward+penalty),done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpUEEiMsqXM2",
        "colab_type": "code",
        "colab": {},
        "outputId": "ec258dbf-b417-432f-f4f2-4ae58679b0e6"
      },
      "source": [
        "env = gameEnv(partial=False,size=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:112: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:113: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:114: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f33bc039898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfL4pNeAqXM4",
        "colab_type": "text"
      },
      "source": [
        "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BwmGkAqqXM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Qnetwork():\n",
        "    def __init__(self,h_size):\n",
        "        #The network recieves a frame from the game, flattened into an array.\n",
        "        #It then resizes it and processes it through four convolutional layers.\n",
        "        self.scalarInput =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
        "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
        "        self.conv1 = slim.conv2d( \\\n",
        "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
        "        self.conv2 = slim.conv2d( \\\n",
        "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
        "        self.conv3 = slim.conv2d( \\\n",
        "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
        "        self.conv4 = slim.conv2d( \\\n",
        "            inputs=self.conv3,num_outputs=h_size,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
        "        \n",
        "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
        "        self.streamAC,self.streamVC = tf.split(self.conv4,2,3)\n",
        "        self.streamA = slim.flatten(self.streamAC)\n",
        "        self.streamV = slim.flatten(self.streamVC)\n",
        "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
        "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
        "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
        "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
        "        self.Value = tf.matmul(self.streamV,self.VW)\n",
        "        \n",
        "        #Then combine them together to get our final Q-values.\n",
        "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
        "        self.predict = tf.argmax(self.Qout,1)\n",
        "        \n",
        "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
        "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
        "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
        "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
        "        \n",
        "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
        "        \n",
        "        self.td_error = tf.square(self.targetQ - self.Q)\n",
        "        self.loss = tf.reduce_mean(self.td_error)\n",
        "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
        "        self.updateModel = self.trainer.minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cph9D3WKqXM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class experience_buffer():\n",
        "    def __init__(self, buffer_size = 50000):\n",
        "        self.buffer = []\n",
        "        self.buffer_size = buffer_size\n",
        "    \n",
        "    def add(self,experience):\n",
        "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
        "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
        "        self.buffer.extend(experience)\n",
        "            \n",
        "    def sample(self,size):\n",
        "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8mtyCxLqXM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def processState(states):\n",
        "    return np.reshape(states,[21168])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3piSZ9iHqXM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def updateTargetGraph(tfVars,tau):\n",
        "    total_vars = len(tfVars)\n",
        "    op_holder = []\n",
        "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
        "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
        "    return op_holder\n",
        "\n",
        "def updateTarget(op_holder,sess):\n",
        "    for op in op_holder:\n",
        "        sess.run(op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFlzTbckqXNC",
        "colab_type": "code",
        "colab": {},
        "outputId": "72969953-9ccf-4442-eef8-a163067d15ac"
      },
      "source": [
        "batch_size = 32 #How many experiences to use for each training step.\n",
        "update_freq = 4 #How often to perform a training step.\n",
        "y = .99 #Discount factor on the target Q-values\n",
        "startE = 1 #Starting chance of random action\n",
        "endE = 0.1 #Final chance of random action\n",
        "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
        "num_episodes = 10000 #How many episodes of game environment to train network with.\n",
        "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
        "max_epLength = 50 #The max allowed length of our episode.\n",
        "load_model = False #Whether to load a saved model.\n",
        "path = \"./dqn\" #The path to save our model to.\n",
        "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
        "tau = 0.001 #Rate to update target network toward primary network\n",
        "\n",
        "tf.reset_default_graph()\n",
        "mainQN = Qnetwork(h_size)\n",
        "targetQN = Qnetwork(h_size)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "trainables = tf.trainable_variables()\n",
        "\n",
        "targetOps = updateTargetGraph(trainables,tau)\n",
        "\n",
        "myBuffer = experience_buffer()\n",
        "\n",
        "#Set the rate of random action decrease. \n",
        "e = startE\n",
        "stepDrop = (startE - endE)/anneling_steps\n",
        "\n",
        "#create lists to contain total rewards and steps per episode\n",
        "jList = []\n",
        "rList = []\n",
        "total_steps = 0\n",
        "\n",
        "#Make a path for our model to be saved in.\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    if load_model == True:\n",
        "        print('Loading Model...')\n",
        "        ckpt = tf.train.get_checkpoint_state(path)\n",
        "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
        "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
        "    for i in range(num_episodes):\n",
        "        episodeBuffer = experience_buffer()\n",
        "        #Reset environment and get first new observation\n",
        "        s = env.reset()\n",
        "        s = processState(s)\n",
        "        d = False\n",
        "        rAll = 0\n",
        "        j = 0\n",
        "        #The Q-Network\n",
        "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
        "            j+=1\n",
        "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
        "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
        "                a = np.random.randint(0,4)\n",
        "            else:\n",
        "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
        "            s1,r,d = env.step(a)\n",
        "            s1 = processState(s1)\n",
        "            total_steps += 1\n",
        "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
        "            \n",
        "            if total_steps > pre_train_steps:\n",
        "                if e > endE:\n",
        "                    e -= stepDrop\n",
        "                \n",
        "                if total_steps % (update_freq) == 0:\n",
        "                    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
        "                    #Below we perform the Double-DQN update to the target Q-values\n",
        "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
        "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
        "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
        "                    doubleQ = Q2[range(batch_size),Q1]\n",
        "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
        "                    #Update the network with our target values.\n",
        "                    _ = sess.run(mainQN.updateModel, \\\n",
        "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
        "                    \n",
        "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
        "            rAll += r\n",
        "            s = s1\n",
        "            \n",
        "            if d == True:\n",
        "\n",
        "                break\n",
        "        \n",
        "        myBuffer.add(episodeBuffer.buffer)\n",
        "        jList.append(j)\n",
        "        rList.append(rAll)\n",
        "        #Periodically save the model. \n",
        "        if i % 1000 == 0:\n",
        "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
        "            print(\"Saved Model\")\n",
        "        if len(rList) % 10 == 0:\n",
        "            print(total_steps,np.mean(rList[-10:]), e)\n",
        "    saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
        "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-27-a690ebb74b4d>:23: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:112: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:113: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n",
            "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:114: DeprecationWarning: `imresize` is deprecated!\n",
            "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
            "Use ``skimage.transform.resize`` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved Model\n",
            "500 2.2 1\n",
            "1000 0.8 1\n",
            "1500 2.1 1\n",
            "2000 1.3 1\n",
            "2500 2.4 1\n",
            "3000 1.2 1\n",
            "3500 1.6 1\n",
            "4000 2.8 1\n",
            "4500 2.6 1\n",
            "5000 1.8 1\n",
            "5500 2.6 1\n",
            "6000 2.9 1\n",
            "6500 0.4 1\n",
            "7000 2.7 1\n",
            "7500 3.1 1\n",
            "8000 1.2 1\n",
            "8500 2.2 1\n",
            "9000 3.1 1\n",
            "9500 1.9 1\n",
            "10000 3.5 1\n",
            "10500 2.7 0.9549999999999828\n",
            "11000 2.2 0.9099999999999655\n",
            "11500 2.2 0.8649999999999483\n",
            "12000 0.2 0.819999999999931\n",
            "12500 2.2 0.7749999999999138\n",
            "13000 2.6 0.7299999999998965\n",
            "13500 2.1 0.6849999999998793\n",
            "14000 2.1 0.639999999999862\n",
            "14500 2.0 0.5949999999998448\n",
            "15000 3.5 0.5499999999998275\n",
            "15500 1.6 0.5049999999998103\n",
            "16000 2.1 0.4599999999998177\n",
            "16500 1.2 0.41499999999982823\n",
            "17000 3.1 0.36999999999983874\n",
            "17500 1.7 0.32499999999984924\n",
            "18000 2.7 0.27999999999985975\n",
            "18500 1.1 0.23499999999986562\n",
            "19000 2.3 0.18999999999986225\n",
            "19500 2.2 0.14499999999985888\n",
            "20000 1.6 0.09999999999985551\n",
            "20500 2.5 0.09999999999985551\n",
            "21000 1.4 0.09999999999985551\n",
            "21500 2.3 0.09999999999985551\n",
            "22000 2.6 0.09999999999985551\n",
            "22500 2.5 0.09999999999985551\n",
            "23000 2.7 0.09999999999985551\n",
            "23500 2.4 0.09999999999985551\n",
            "24000 2.0 0.09999999999985551\n",
            "24500 3.5 0.09999999999985551\n",
            "25000 2.2 0.09999999999985551\n",
            "25500 2.9 0.09999999999985551\n",
            "26000 3.7 0.09999999999985551\n",
            "26500 2.9 0.09999999999985551\n",
            "27000 3.4 0.09999999999985551\n",
            "27500 3.9 0.09999999999985551\n",
            "28000 5.4 0.09999999999985551\n",
            "28500 5.2 0.09999999999985551\n",
            "29000 4.7 0.09999999999985551\n",
            "29500 5.9 0.09999999999985551\n",
            "30000 5.5 0.09999999999985551\n",
            "30500 5.5 0.09999999999985551\n",
            "31000 6.7 0.09999999999985551\n",
            "31500 6.2 0.09999999999985551\n",
            "32000 4.8 0.09999999999985551\n",
            "32500 7.6 0.09999999999985551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cbf4a1aff607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mtargetQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdoubleQ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mend_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;31m#Update the network with our target values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateModel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalarInput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtargetQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrainBatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mupdateTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetOps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Set the target network to be equal to the primary network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeRPcATCqXNE",
        "colab_type": "text"
      },
      "source": [
        "## Other examples\n",
        "\n",
        "https://www.youtube.com/watch?v=gn4nRCC9TwQ\n",
        "\n",
        "https://www.youtube.com/watch?v=wBrwN4dS-DA\n",
        "\n",
        "https://www.youtube.com/watch?v=n7370dzCZ1o\n",
        "\n",
        "https://www.youtube.com/watch?v=fv-oFPAqSZ4"
      ]
    }
  ]
}
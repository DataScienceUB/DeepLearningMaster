{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression model is one of the simplest regression models. It assumes linear relationship between $X$ and $Y$. The output equation is defined as follows:\n",
    "\n",
    "$$\\hat{y} = WX + b$$\n",
    "\n",
    "The *Advertising data set* (from \"*An Introduction to Statistical Learning*\", textbook by Gareth James, Robert Tibshirani, and Trevor Hastie) consists of the sales of a product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. \n",
    "\n",
    "Objective: to determine if there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. \n",
    "\n",
    "We want to train an **inference model**, a series of mathematical expressions we want to apply to our data that depends on a series of parameters. The values of parameters change through training in order for the model to learn and adjust its output.\n",
    "\n",
    "The training loop is:\n",
    "+ Initialize the model parameters to some values.\n",
    "+ Read the training data (for each example), possibly using randomization strategies in order to assure that training is stochastic.\n",
    "+ Execute the inference model on the training data, getting for each training example the model output with the parameter values. \n",
    "+ Compute the loss.\n",
    "+ Adjuts the model parameters.\n",
    "\n",
    "We will repeat this process a number of times, according to the learning rate.\n",
    "\n",
    "After the training we will apply an evaluation phase. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data.\n",
    "\n",
    "import numpy as np\n",
    "data = pd.read_csv('data/Advertising.csv',index_col=0)\n",
    "\n",
    "train_X = data[['TV']].values \n",
    "\n",
    "train_Y = data.Sales.values \n",
    "train_Y = train_Y[:,np.newaxis]\n",
    "n_samples = train_X.shape[0]\n",
    "print(n_samples)\n",
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data visualization \n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_ylabel('Results',\n",
    "              rotation=0,\n",
    "              ha='right', # horizontalalignment\n",
    "              ma='left', # multiline alignment\n",
    "             )\n",
    "ax.set_xlabel('TV')\n",
    "fig.set_facecolor('#EAEAF2')\n",
    "ax.plot(train_X, train_Y, 'o', color=sns.xkcd_rgb['pale red'], alpha=0.6,label='Original data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.global_variables_initializer()\n",
    "\n",
    "#  Training Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 100\n",
    "\n",
    "# Define tf Graph Inputs\n",
    "X = tf.placeholder(\"float\",[None,1])\n",
    "y = tf.placeholder(\"float\",[None,1])\n",
    "\n",
    "# Create Model variables \n",
    "# Set model weights\n",
    "W = tf.Variable(np.random.randn(), name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "# Construct a linear model\n",
    "y_pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# Minimize the squared errors\n",
    "cost = tf.reduce_sum(tf.pow(y_pred - y,2))/(2*n_samples) #L2 loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    cost_plot = []\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(optimizer, \n",
    "                 feed_dict={X: train_X, y: train_Y})       \n",
    "        cost_plot.append(sess.run(cost, \n",
    "                                  feed_dict={X: train_X, y:train_Y}))\n",
    "                \n",
    "    print(\"\")\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"cost=\", sess.run(cost, \n",
    "                            feed_dict={X: train_X, y: train_Y}), \\\n",
    "          \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.set_ylabel('Results',\n",
    "              rotation=0,\n",
    "              ha='right', # horizontalalignment\n",
    "              ma='left', # multiline alignment\n",
    "             )\n",
    "    ax.set_xlabel('TV')\n",
    "    fig.set_facecolor('#EAEAF2')\n",
    "    ax.plot(train_X, \n",
    "            train_Y, 'o', \n",
    "            color=sns.xkcd_rgb['pale red'], \n",
    "            alpha=0.6,label='Original data')\n",
    "    plt.plot(train_X, \n",
    "             sess.run(W) * train_X + sess.run(b), \n",
    "             label='Fitted line')\n",
    "    plt.show()\n",
    "    \n",
    "    x = range(len(cost_plot))\n",
    "    plt.plot(x,  np.sqrt(cost_plot))\n",
    "    plt.show()\n",
    "    print(cost_plot[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Tune the learning parameters of the previous example in order to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "Let's use three features as input vector : TV, Radio, Newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1e-2\n",
    "training_epochs = 2000\n",
    "display_step = 200\n",
    "\n",
    "import numpy as np\n",
    "data = pd.read_csv('data/Advertising.csv',index_col=0)\n",
    "train_X = data[['TV','Radio','Newspaper']].values\n",
    "train_Y = data.Sales.values \n",
    "train_Y = train_Y[:,np.newaxis]\n",
    "n_samples = train_X.shape[0]\n",
    "print(n_samples)\n",
    "print(train_X.shape, train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tf Graph Inputs\n",
    "X = tf.placeholder(\"float\",[None,3])\n",
    "y = tf.placeholder(\"float\",[None,1])\n",
    "\n",
    "# Create Model variables \n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([3, 1]),name=\"bias\")\n",
    "b = tf.Variable(np.random.randn(), name=\"bias\")\n",
    "\n",
    "# Construct a multidimensional linear model\n",
    "y_pred = tf.matmul(X, W) + b\n",
    "\n",
    "# Minimize the squared errors\n",
    "cost = tf.reduce_sum(tf.pow(y_pred-y,2))/(2*n_samples) #L2 loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost) \n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        sess.run(optimizer, feed_dict={X: train_X, y: train_Y})\n",
    "        \n",
    "        #Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch: \", '%04d' % (epoch+1), \"\\n cost= \", sess.run(cost, feed_dict={X: train_X, y: train_Y}), \\\n",
    "          \"\\n W= \", sess.run(W), \"\\n b= \", sess.run(b), \"\\n\")\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"cost= \\n\", sess.run(cost, feed_dict={X: train_X, y: train_Y}), \\\n",
    "          \"\\n W= \\n\", sess.run(W), \"\\n b= \\n\", sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `tf` helpers\n",
    "\n",
    "Updating of parameters through many training cycles can be dangerous (f.e. if your computer lose power). The `tf.train.Saver` class can save the graph variables for later reuse. \n",
    "\n",
    "```python\n",
    "...\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    for step in range(training_steps):\n",
    "        sess.run(...)\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, 'my-model', global_step=step)\n",
    "    # evaluation\n",
    "    saver.safe(sess, 'my-model', global_step=training_steps)\n",
    "    sess.close()\n",
    "```\n",
    "\n",
    "If we want to recover the training from a certain point we should use the `tf.train.get_checkpoint_state` method, which will verify that there is a checkpoint saved, and the `tf.train.Saver.restore` method to recover the variable values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Logistic regression.\n",
    "\n",
    "Complete the following code.\n",
    "\n",
    "The linear model predicts a constinous value. Now we are going to write a model that can answer yes/no question: **logistic regression**:\n",
    "\n",
    "$$ \\hat{y} = \\frac{1}{1+ e^{-(WX + b)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# same params and variables initialization as log reg.\n",
    "W = tf.Variable(tf.zeros([5, 1]), name=\"weights\")\n",
    "b = tf.Variable(0., name=\"bias\")\n",
    "\n",
    "# your code here: write a function called 'inference' to implement the logistic regression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cross-entropy** loss function is the best suited for logistic regression:\n",
    "\n",
    "$$ L = -\\sum_i (y_i \\cdot \\log (\\hat{y}_i) + (1 - y_i) \\cdot  \\log (1 - \\hat{y}_i)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, Y):\n",
    "    \n",
    "    #your code here\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to read the survivor Titanic dataset. The model will have to infer, based on the passenger age, sex and ticket class if the passenger survived or not. We will create a batch to read many rows packed in a single tensor for computing the inference efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    \n",
    "    #\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([os.path.join(os.getcwd(), file_name)])\n",
    "\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "\n",
    "    # decode_csv will convert a Tensor from type string (the text line) in\n",
    "    # a tuple of tensor columns with the specified defaults, which also\n",
    "    # sets the data type for each column\n",
    "    \n",
    "    decoded = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "\n",
    "    # batch actually reads the file and loads \"batch_size\" rows in a single tensor\n",
    "    return tf.train.shuffle_batch(decoded,\n",
    "                                  batch_size=batch_size,\n",
    "                                  capacity=batch_size * 50,\n",
    "                                  min_after_dequeue=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have *categorical* features in this dataset (`ticket_class, gender`) and we need to convert them to numbers. To this end we can convert each categorical feature to $N$ boolean features that represent each possible value. \n",
    "\n",
    "In case of categorical values with to values it is enough to use a binary feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs():\n",
    "    passenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = \\\n",
    "        read_csv(100, \"data/train_titanic.csv\", [[0.0], [0.0], [0], [\"\"], [\"\"], [0.0], [0.0], [0.0], [\"\"], [0.0], [\"\"], [\"\"]])\n",
    "\n",
    "    # convert categorical data\n",
    "    is_first_class = tf.to_float(tf.equal(pclass, [1]))\n",
    "    is_second_class = tf.to_float(tf.equal(pclass, [2]))\n",
    "    is_third_class = tf.to_float(tf.equal(pclass, [3]))\n",
    "\n",
    "    gender = tf.to_float(tf.equal(sex, [\"female\"]))\n",
    "\n",
    "    # Finally we pack all the features in a single matrix;\n",
    "    # We then transpose to have a matrix with one example per row and one feature per column.\n",
    "    features = tf.transpose(tf.stack([is_first_class, is_second_class, is_third_class, gender, age]))\n",
    "    survived = tf.reshape(survived, [100, 1])\n",
    "\n",
    "    return features, survived\n",
    "\n",
    "\n",
    "def train(total_loss):   \n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    predicted = tf.cast(inference(X) > 0.5, tf.float32)\n",
    "    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n",
    "\n",
    "# Launch the graph in a session, setup boilerplate\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    X, Y = inputs()\n",
    "\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    # actual training loop\n",
    "    training_steps = 5000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        # for debugging and learning purposes, see how the loss gets decremented thru training steps\n",
    "        if step % 100 == 0:\n",
    "            print(\"loss: \", sess.run([total_loss]))\n",
    "\n",
    "    evaluate(sess, X, Y)\n",
    "\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'tf' Neural Network from scratch\n",
    "\n",
    "Let's classify handwritten digits:\n",
    "\n",
    "![alt text](images/mnistExamples.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MINST data\n",
    "# The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), \n",
    "# 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation).\n",
    "\n",
    "# Both the training set and test set contain images and their corresponding labels; for example the \n",
    "# training images are mnist.train.images and the training labels are mnist.train.labels.\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "plt.imshow(mnist.train.images[0].reshape((28, 28)), interpolation='nearest', cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"Class: \", np.argmax(mnist.train.labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input    = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes  = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = multilayer_perceptron(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([train_op, loss_op], feed_dict={X: batch_x,\n",
    "                                                            Y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

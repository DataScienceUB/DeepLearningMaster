{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceUB/DeepLearningMaster2019/blob/master/5.%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yxTtoHZdQEzw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# `keras`\n",
        "\n",
        "> Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cGkk9uf_QZ-o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "514c4ff2-016e-4a0a-8ed3-e8c286c4d918"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.VERSION)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-bme-ciBSrmX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definition of your model and optimization process"
      ]
    },
    {
      "metadata": {
        "id": "KXtqznrhQEzz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. "
      ]
    },
    {
      "metadata": {
        "id": "y2RzbfJIQpHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from keras.models import Sequential\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIHoW-hTQEz0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stacking layers is as easy as ``.add()``:\n"
      ]
    },
    {
      "metadata": {
        "id": "npq3S6jtQy-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Adds a densely-connected layer with 64 units to the model:\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Add another:\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "# Add a softmax layer with 10 output units:\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvZW0RY5QEz1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once your model looks good, configure its learning process with ``.compile()``:"
      ]
    },
    {
      "metadata": {
        "id": "Jb-1515bRtYR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b7a16352-391f-4217-ab61-aa3f85da0996"
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "# Adds a densely-connected layer with 64 units to the model:\n",
        "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
        "# Add another:\n",
        "layers.Dense(64, activation='relu'),\n",
        "# Add a softmax layer with 10 output units:\n",
        "layers.Dense(10, activation='softmax')])\n",
        "\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "63Y89VPDR2Yn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tf.keras.Model.compile` takes three important arguments:\n",
        "\n",
        "* `optimizer`: This object specifies the training procedure. Pass it optimizer\n",
        "  instances from the `tf.train` module, such as\n",
        "  `tf.train.AdamOptimizer`, `tf.train.RMSPropOptimizer`, or\n",
        "  `tf.train.GradientDescentOptimizer`.\n",
        "* `loss`: The function to minimize during optimization. Common choices include\n",
        "  mean square error (`mse`), `categorical_crossentropy`, and\n",
        "  `binary_crossentropy`. Loss functions are specified by name or by\n",
        "  passing a callable object from the `tf.keras.losses` module.\n",
        "* `metrics`: Used to monitor training. These are string names or callables from\n",
        "  the `tf.keras.metrics` module."
      ]
    },
    {
      "metadata": {
        "id": "88OWboaDQEz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Other examples:\n",
        "\n",
        "```python\n",
        "# Configure a model for mean-squared error regression.\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
        "              loss='mse',       # mean squared error\n",
        "              metrics=['mae'])  # mean absolute error\n",
        "\n",
        "# Configure a model for categorical classification.\n",
        "model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
        "              loss=tf.keras.losses.categorical_crossentropy,\n",
        "              metrics=[tf.keras.metrics.categorical_accuracy])\n",
        "```\n",
        "\n",
        "If you need to, you can further configure your optimizer.\n",
        "\n",
        "```Python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "IJuhl4rrS2wE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Input and model fitting.\n",
        "\n",
        "For small datasets, use in-memory [NumPy](https://www.numpy.org/)\n",
        "arrays to train and evaluate a model. \n",
        "\n",
        "The model is \"fit\" to the training data\n",
        "using the `fit` method:"
      ]
    },
    {
      "metadata": {
        "id": "AAAWNGGWTCjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "a08bebe5-eb78-47c0-bcd7-3c19a290d015"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.random.random((1000, 32))\n",
        "labels = np.random.random((1000, 10))\n",
        "\n",
        "model.fit(data, labels, epochs=10, batch_size=32)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 136us/sample - loss: 11.5512 - acc: 0.0930\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 50us/sample - loss: 11.5054 - acc: 0.1160\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 47us/sample - loss: 11.4961 - acc: 0.1290\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 49us/sample - loss: 11.4902 - acc: 0.1360\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 45us/sample - loss: 11.4849 - acc: 0.1330\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 48us/sample - loss: 11.4793 - acc: 0.1370\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 48us/sample - loss: 11.4744 - acc: 0.1620\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 47us/sample - loss: 11.4694 - acc: 0.1550\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 52us/sample - loss: 11.4644 - acc: 0.1550\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 46us/sample - loss: 11.4597 - acc: 0.1670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb0135c2fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "tZiYjZGVTT8A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`tensorflow.keras.Model.fit` takes three important arguments:\n",
        "\n",
        "* `epochs`: Training is structured into *epochs*. \n",
        "* `batch_size`: This integer\n",
        "  specifies the size of each batch. Be aware that the last batch may be smaller\n",
        "  if the total number of samples is not divisible by the batch size.\n",
        "* `validation_data`: Passing this argument—a tuple of inputs\n",
        "  and labels—allows the model to display the loss and metrics in inference mode\n",
        "  for the passed data, at the end of each epoch.\n",
        "\n",
        "Here's an example using `validation_data`:"
      ]
    },
    {
      "metadata": {
        "id": "zd8UjPlyTd5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "dad6ff97-fc04-410d-d123-a2aac8a4dc6e"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.random.random((1000, 32))\n",
        "labels = np.random.random((1000, 10))\n",
        "\n",
        "val_data = np.random.random((100, 32))\n",
        "val_labels = np.random.random((100, 10))\n",
        "\n",
        "model.fit(data, labels, epochs=10, batch_size=32,\n",
        "          validation_data=(val_data, val_labels))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1000 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 87us/sample - loss: 11.4384 - acc: 0.1160 - val_loss: 11.4166 - val_acc: 0.0600\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 52us/sample - loss: 11.4241 - acc: 0.1210 - val_loss: 11.4111 - val_acc: 0.0800\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 52us/sample - loss: 11.4153 - acc: 0.1270 - val_loss: 11.4139 - val_acc: 0.0900\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 53us/sample - loss: 11.4097 - acc: 0.1460 - val_loss: 11.4135 - val_acc: 0.1100\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 54us/sample - loss: 11.4034 - acc: 0.1410 - val_loss: 11.4123 - val_acc: 0.1100\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 53us/sample - loss: 11.3989 - acc: 0.1610 - val_loss: 11.4182 - val_acc: 0.1200\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 49us/sample - loss: 11.3946 - acc: 0.1510 - val_loss: 11.4206 - val_acc: 0.1100\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 49us/sample - loss: 11.3912 - acc: 0.1690 - val_loss: 11.4222 - val_acc: 0.1300\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 52us/sample - loss: 11.3845 - acc: 0.1790 - val_loss: 11.4248 - val_acc: 0.0800\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 51us/sample - loss: 11.3809 - acc: 0.1720 - val_loss: 11.4279 - val_acc: 0.1200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb00e0ac400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "jRE5P97jTvV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have to process large datasets you must use `tf` Datasets API. `feed-dict` is the slowest possible way to pass information to TensorFlow.\n",
        "\n",
        "A `Dataset` can be used to represent an input pipeline as a collection of elements (nested structures of tensors) and a \"logical plan\" of transformations that act on those elements.\n",
        "\n",
        "In order to use a Dataset we need three steps:\n",
        "\n",
        "+ Importing Data. Create a `Dataset` instance from some data.\n",
        "+ Create an Iterator. By using the created dataset to make an `Iterator` instance to iterate through the dataset.\n",
        "+ Consuming Data. By using the created iterator we can get the elements from the dataset to feed the model.\n"
      ]
    },
    {
      "metadata": {
        "id": "Qb_E7q45UAbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2087cb22-b2f3-45aa-f283-a3111dc42b51"
      },
      "cell_type": "code",
      "source": [
        "# Instantiates a toy dataset instance:\n",
        "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "\n",
        "\n",
        "# With the Dataset API we can use the method batch(BATCH_SIZE) that \n",
        "# automatically batches the dataset with the provided size. \n",
        "# The default value is one.\n",
        "dataset = dataset.batch(32)\n",
        "\n",
        "\n",
        "# Using .repeat() we can specify the number of times we want the dataset to be \n",
        "# iterated. If no parameter is passed it will loop forever, usually is good to \n",
        "# just loop forever and directly control the number of epochs with a standard \n",
        "# loop.\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
        "model.fit(dataset, epochs=4, steps_per_epoch=30)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "30/30 [==============================] - 0s 5ms/step - loss: 11.3931 - acc: 0.1729\n",
            "Epoch 2/4\n",
            "30/30 [==============================] - 0s 1ms/step - loss: 11.3676 - acc: 0.1806\n",
            "Epoch 3/4\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 11.3319 - acc: 0.1763\n",
            "Epoch 4/4\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 11.3313 - acc: 0.1870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb00e0aceb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "oE3xO_piVH8P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can, of course, initialise our dataset with some tensor:\n",
        "\n",
        "```python\n",
        "# using a tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))\n",
        "```\n",
        "\n",
        "There are some other options: from a `Placeholder`, from a generator, from a `csv` file, etc.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1mAUEQbnYImc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate and predict\n",
        "\n",
        "The `tf.keras.Model.evaluate` and `tf.keras.Model.predict` methods can use NumPy\n",
        "data and a `tf.data.Dataset`.\n",
        "\n",
        "To *evaluate* the inference-mode loss and metrics for the data provided:"
      ]
    },
    {
      "metadata": {
        "id": "N0x8ULFQYJ1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "795d5e92-acaf-41b1-a106-7c0eb7af5db0"
      },
      "cell_type": "code",
      "source": [
        "data = np.random.random((1000, 32))\n",
        "labels = np.random.random((1000, 10))\n",
        "\n",
        "model.evaluate(data, labels, batch_size=32)\n",
        "\n",
        "model.evaluate(dataset, steps=30)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 24us/sample - loss: 11.4338 - acc: 0.1060\n",
            "30/30 [==============================] - 0s 2ms/step - loss: 11.3706 - acc: 0.1927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.370595582326253, 0.19270833]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "pSryJ0YXYR6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And to *predict* the output of the last layer in inference for the data provided,\n",
        "as a NumPy array:"
      ]
    },
    {
      "metadata": {
        "id": "r41ioMVWYTBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ed764f5-3905-4a98-89a7-7dbf2ea487f8"
      },
      "cell_type": "code",
      "source": [
        "result = model.predict(data, batch_size=32)\n",
        "print(result.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MZi4CXR7QEz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example: MNIST  MLP"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:11:33.329607Z",
          "start_time": "2019-03-05T12:10:46.072748Z"
        },
        "id": "yARoL5izQEz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "c8f2b680-37eb-4300-9733-0ef59c7653c3"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "\n",
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "nb_epoch = 10\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test  = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(784,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# print model characteristics\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    Y_train,\n",
        "                    batch_size=batch_size, \n",
        "                    epochs=nb_epoch,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('\\n')\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.3285 - acc: 0.9374 - val_loss: 0.1946 - val_acc: 0.9677\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 166us/step - loss: 0.3242 - acc: 0.9381 - val_loss: 0.1922 - val_acc: 0.9693\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.3269 - acc: 0.9390 - val_loss: 0.1913 - val_acc: 0.9681\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.3257 - acc: 0.9384 - val_loss: 0.1944 - val_acc: 0.9692\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.3228 - acc: 0.9404 - val_loss: 0.2087 - val_acc: 0.9678\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.3333 - acc: 0.9386 - val_loss: 0.2090 - val_acc: 0.9673\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.3309 - acc: 0.9392 - val_loss: 0.1990 - val_acc: 0.9694\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.3366 - acc: 0.9376 - val_loss: 0.2032 - val_acc: 0.9679\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 10s 169us/step - loss: 0.3314 - acc: 0.9384 - val_loss: 0.2010 - val_acc: 0.9710\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.3278 - acc: 0.9418 - val_loss: 0.1990 - val_acc: 0.9715\n",
            "\n",
            "\n",
            "Test score: 0.1990005832468405\n",
            "Test accuracy: 0.9715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HiGrUqawaMYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The `dropout layer`\n",
        "\n",
        "Dropout is a a technique used to tackle overfitting . The `Dropout` method in `keras.layers` module takes in a float between 0 and 1, which is the fraction of the neurons to drop. \n",
        "\n",
        "A dropout layer randomly sets some of the dimensions of your input vector to be zero with some probability $𝑘𝑒𝑒𝑝_{𝑝𝑟𝑜𝑏}$. A dropout layer does not have any trainable parameters i.e. nothing gets updated during backward pass of backpropagation.\n",
        "\n",
        "To ensure that expected sum of vectors fed to this layer remains the same if no dropout was applied, the remaining dimensions which are not set to zero are scaled by $1/𝑘𝑒𝑒𝑝_{𝑝𝑟𝑜𝑏}$.\n",
        "\n",
        "Dropout is only applied during training, and you need to rescale the remaining neuron activations. "
      ]
    },
    {
      "metadata": {
        "id": "51CiIGkHQE0H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The ``merge`` layer\n",
        "\n",
        "Multiple Sequential instances can be merged into a single output via a ``Merge`` layer. The output is a layer that can be added as first layer in a new ``Sequential`` model. For instance, here's a model with two separate input branches getting merged:\n",
        "\n",
        "```python\n",
        "from keras.layers import Merge\n",
        "\n",
        "left_branch = Sequential()\n",
        "left_branch.add(Dense(32, input_dim=784))\n",
        "\n",
        "right_branch = Sequential()\n",
        "right_branch.add(Dense(32, input_dim=784))\n",
        "\n",
        "merged = Merge([left_branch, right_branch], mode='concat')\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(merged)\n",
        "final_model.add(Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/merge.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "lUoptiW5QE0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Merge layers: ``add``, ``subtract``, ...\n",
        "\n",
        "\n",
        "The Merge layer supports a number of pre-defined modes:\n",
        "\n",
        "+ `sum` (default): element-wise sum\n",
        "concat: tensor concatenation. You can specify the concatenation axis via the argument concat_axis.\n",
        "+ `mul`: element-wise multiplication\n",
        "+ `ave`: tensor average\n",
        "+ `dot`: dot product. You can specify which axes to reduce along via the argument dot_axes.\n",
        "+ `cos`: cosine proximity between vectors in 2D tensors."
      ]
    },
    {
      "metadata": {
        "id": "Wdcv_yvcQE0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras functional API\n",
        "\n",
        "The Keras ``functional`` API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "\n",
        "The ``Sequential`` model is probably a better choice to implement such a network, but it helps to start with something really simple.\n",
        "\n",
        "Using ``Model`` class:\n",
        "\n",
        "+ A layer instance is callable (on a tensor), and it returns a tensor\n",
        "+ ``Input`` tensor(s) and output tensor(s) can then be used to define a ``Model``\n",
        "+ Such a model can be trained just like Keras Sequential models.\n",
        "\n",
        "```python\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# this returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# this creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(input=inputs, output=predictions)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "model.fit(data, labels)  # starts training\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "ZLz7HfbNeBPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "data = X_train/255\n",
        "labels = np_utils.to_categorical(y_train, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6WK7G-mdiL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "6229f7ae-088d-4302-e55a-b2fe620d9797"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# this returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# this creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(input=inputs, output=predictions)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(data, \n",
        "          labels,\n",
        "          batch_size=128, \n",
        "          epochs=5,\n",
        "          verbose=1)  # starts training"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.3748 - acc: 0.8941\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1712 - acc: 0.9493\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1257 - acc: 0.9629\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0999 - acc: 0.9699\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0828 - acc: 0.9745\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f21af3f2eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "YHiwfriqQE0S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All models are callable, just like layers.\n",
        "\n",
        "With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the architecture of the model, you are also re-using its weights.\n",
        "\n",
        "```python\n",
        "x = Input(shape=(784,))\n",
        "# this works, and returns the 10-way softmax we defined above.\n",
        "y = model(x)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "B8BTfwxVQE0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Siamese MLP on pairs of digits from the MNIST \n",
        "\n",
        "(Source: https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py)\n",
        "\n",
        "Siamese networks are commonly used in image comparison applications such as face or signature verification. They can also be used in language processing, times series analysis, etc.\n",
        "\n",
        "In a typical Siamese network a large part of the network is duplicated at the base to allow multiple inputs to go through identical layers. \n",
        "\n",
        "This example shows how to teach a neural network to map an image from the MNIST dataset to a 2D point, while trying to minimize the distance between points of the same class and maximize the distance between points of different classes."
      ]
    },
    {
      "metadata": {
        "id": "mnmUEne_QE0V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Siamese network architecture is a way of learning how to embed samples into lower-dimensions based on similarity computed with features learned by a feature network.\n",
        "\n",
        "The feature network is the architecture we intend to fine-tune in this setting. \n",
        "\n",
        "Let's suppose we want to embed images. Given two images $X_1$ and $X_2$, we feed into the feature network $G_W$ and compute corresponding feature vectors $G_W(X_1)$ and $G_W(X_2)$. The final layer computes pair-wise distance between computed features $E_W = || G_W(X_1) - G_W(X_2) ||_{1}$ and final loss layer $L$ considers whether these two images are from the same class (label $1$) or not (label $0$).\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/siamese1.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "9NVK0jyCQE0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the original [paper](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf) it was proposed the **Contrastive Loss Function**: \n",
        "\n",
        "$$ L(W,(Y,X_1,X_2)^i) = (1 - Y) \\times L_S(E_W(X_1,X_2)^i) + Y \\times L_D(E_W(X_1,X_2)^i) $$\n",
        "\n",
        "where $L_S$ is the partial loss function for a \"same-class\" pair and $L_D$ is the partial loss function for a \"different-class\" pair.\n",
        "\n",
        "$L_S$ and $L_D$ should be designed in such a way that the minimization of $L$ will decrease the distance in the embedding space of \"same-class\" pairs and increase it in the case of \"different-class\" pairs:\n",
        "\n",
        "$$ L_S = \\frac{1}{2} E_W^2 $$\n",
        "$$ L_D = \\frac{1}{2} \\{ \\mbox{max }(0,1-E_W) \\}^2 $$"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:22.544377Z",
          "start_time": "2019-03-05T12:11:49.850575Z"
        },
        "id": "tgINnEdyQE0Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "ff911bf7-aa5e-4c8a-8e23-d14f3b0a3811"
      },
      "cell_type": "code",
      "source": [
        "'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n",
        "It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n",
        "output of the shared network and by optimizing the contrastive loss (see paper\n",
        "for mode details).\n",
        "[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "Gets to 99.5% test accuracy after 20 epochs.\n",
        "3 seconds per epoch on a Titan X GPU\n",
        "'''\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Input, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) \n",
        "                  + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
        "    for d in range(10):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, 10)\n",
        "            dn = (d + inc) % 10\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_base_network(input_dim):\n",
        "    '''\n",
        "    Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    seq = Sequential()\n",
        "    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(2, activation=None,name='emb'))\n",
        "    return seq\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return labels[predictions.ravel() < 0.5].mean()\n",
        "\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "input_dim = 784\n",
        "nb_epoch = 10\n",
        "\n",
        "# create training+test positive and negative pairs\n",
        "digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n",
        "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
        "\n",
        "digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
        "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
        "\n",
        "# network definition\n",
        "base_network = create_base_network(input_dim)\n",
        "\n",
        "input_a = Input(shape=(input_dim,))\n",
        "input_b = Input(shape=(input_dim,))\n",
        "\n",
        "# because we re-use the same instance `base_network`,\n",
        "# the weights of the network\n",
        "# will be shared across the two branches\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "model = Model(inputs=[input_a, input_b], outputs=distance)\n",
        "\n",
        "# train\n",
        "rms = RMSprop()\n",
        "model.compile(loss=contrastive_loss, optimizer=rms)\n",
        "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
        "          batch_size=128,\n",
        "          epochs=nb_epoch)\n",
        "\n",
        "# compute final accuracy on training and test sets\n",
        "pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "tr_acc = compute_accuracy(pred, tr_y)\n",
        "pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
        "te_acc = compute_accuracy(pred, te_y)\n",
        "\n",
        "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 108400 samples, validate on 17820 samples\n",
            "Epoch 1/10\n",
            "108400/108400 [==============================] - 3s 30us/step - loss: 0.1144 - val_loss: 0.0743\n",
            "Epoch 2/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0738 - val_loss: 0.0612\n",
            "Epoch 3/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0607 - val_loss: 0.0540\n",
            "Epoch 4/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0530 - val_loss: 0.0492\n",
            "Epoch 5/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0474 - val_loss: 0.0465\n",
            "Epoch 6/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0437 - val_loss: 0.0449\n",
            "Epoch 7/10\n",
            "108400/108400 [==============================] - 3s 25us/step - loss: 0.0406 - val_loss: 0.0448\n",
            "Epoch 8/10\n",
            "108400/108400 [==============================] - 3s 28us/step - loss: 0.0385 - val_loss: 0.0427\n",
            "Epoch 9/10\n",
            "108400/108400 [==============================] - 3s 27us/step - loss: 0.0362 - val_loss: 0.0410\n",
            "Epoch 10/10\n",
            "108400/108400 [==============================] - 3s 27us/step - loss: 0.0345 - val_loss: 0.0415\n",
            "* Accuracy on training set: 99.29%\n",
            "* Accuracy on test set: 98.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g9EjqnG1QE0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/siameseresult.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "wePbQDmIQE0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-input and multi-output models\n",
        "\n",
        "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n",
        "\n",
        "Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc. \n",
        "\n",
        "The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n",
        "\n",
        "Here's what our model looks like:\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/multi.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "H2AXa03MQE0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's implement it with the functional API.\n",
        "\n",
        "The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:42.969916Z",
          "start_time": "2019-03-05T12:12:42.792604Z"
        },
        "id": "O-oqItqTQE0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
        "# note that we can name any layer by passing it a \"name\" argument.\n",
        "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
        "\n",
        "# this embedding layer will encode the input sequence\n",
        "# into a sequence of dense 512-dimensional vectors.\n",
        "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
        "\n",
        "# a LSTM will transform the vector sequence into a single vector,\n",
        "# containing information about the entire sequence\n",
        "lstm_out = LSTM(32)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6M9kcSHQE0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:44.549821Z",
          "start_time": "2019-03-05T12:12:44.536473Z"
        },
        "id": "M1qQjL_aQE0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KEXrfVYIQE0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:46.193047Z",
          "start_time": "2019-03-05T12:12:46.146507Z"
        },
        "id": "jRvBLeQIQE0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
        "x = concatenate([lstm_out, auxiliary_input])\n",
        "\n",
        "# we stack a deep fully-connected network on top\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# and finally we add the main logistic regression layer\n",
        "main_output = Dense(1, activation='sigmoid', name='main_output')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BMdGUKqKQE0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This defines a model with two inputs and two outputs:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:47.070464Z",
          "start_time": "2019-03-05T12:12:47.067696Z"
        },
        "id": "eqt6hoSGQE0u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cDwUwM-2QE0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:48.060647Z",
          "start_time": "2019-03-05T12:12:48.012672Z"
        },
        "id": "40m66hYDQE00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "              loss_weights=[1., 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUnLWYuUQE03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can train the model by passing it lists of input arrays and target arrays:\n",
        "\n",
        "```python\n",
        "model.fit([headline_data, additional_data], [labels, labels],\n",
        "          nb_epoch=50, batch_size=32)\n",
        "```\n",
        "Since our inputs and outputs are named (we passed them a \"name\" argument), We could also have compiled the model via:\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
        "              loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
        "\n",
        "# and trained it via:\n",
        "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
        "          {'main_output': labels, 'aux_output': labels},\n",
        "          nb_epoch=50, batch_size=32)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "n8PVcBwPQE04",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shared layers\n",
        "\n",
        "Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n",
        "\n",
        "Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\n",
        "\n",
        "One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and adds a logistic regression of top, outputting a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\n",
        "\n",
        "Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\n",
        "\n",
        "Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (140, 256), i.e. a sequence of 140 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters)."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:50.156057Z",
          "start_time": "2019-03-05T12:12:50.150288Z"
        },
        "id": "8ceF9p0YQE06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "tweet_a = Input(shape=(140, 256))\n",
        "tweet_b = Input(shape=(140, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cICM7c8JQE0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:51.729970Z",
          "start_time": "2019-03-05T12:12:51.421564Z"
        },
        "id": "ooh5XHXhQE0_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this layer can take as input a matrix\n",
        "# and will return a vector of size 64\n",
        "shared_lstm = LSTM(64)\n",
        "\n",
        "# when we reuse the same layer instance\n",
        "# multiple times, the weights of the layer\n",
        "# are also being reused\n",
        "# (it is effectively *the same* layer)\n",
        "encoded_a = shared_lstm(tweet_a)\n",
        "encoded_b = shared_lstm(tweet_b)\n",
        "\n",
        "# we can then concatenate the two vectors:\n",
        "merged_vector = concatenate([encoded_a, encoded_b],axis=-1)\n",
        "\n",
        "# and add a logistic regression on top\n",
        "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
        "\n",
        "# we define a trainable model linking the\n",
        "# tweet inputs to the predictions\n",
        "model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5. Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataScienceUB/DeepLearningMaster2019/blob/master/5.%20Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9vmi4BI8xhqB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# `keras`\n",
        "\n",
        "> Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3J26QaB2xhqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. \n",
        "\n",
        "```Python\n",
        "from keras.models import Sequential\n",
        "model = Sequential()\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "ebxU-pLUxhqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stacking layers is as easy as ``.add()``:\n",
        "\n",
        "```Python\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "model.add(Dense(output_dim=64, input_dim=100))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dense(output_dim=10))\n",
        "model.add(Activation(\"softmax\"))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "tAvnheWxxhqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once your model looks good, configure its learning process with ``.compile()``:\n",
        "\n",
        "```Python\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "ZZZ5FogYxhqF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you need to, you can further configure your optimizer.\n",
        "\n",
        "```Python\n",
        "from keras.optimizers import SGD\n",
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "88vCj-cuxhqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can now iterate on your training data in batches:\n",
        "\n",
        "```Python\n",
        "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
        "```\n",
        "\n",
        "Evaluate your performance in one line:\n",
        "```Python\n",
        "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "VR56V-vnxhqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Or generate predictions on new data:\n",
        "\n",
        "```Python\n",
        "classes = model.predict_classes(X_test, batch_size=32)\n",
        "proba = model.predict_proba(X_test, batch_size=32)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "oEkcjRu4xhqH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example: MNIST  MLP"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:11:33.329607Z",
          "start_time": "2019-03-05T12:10:46.072748Z"
        },
        "id": "MweDOphGxhqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1112
        },
        "outputId": "56a5f743-08c7-46fb-9436-7919e4304d5f"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Trains a simple deep NN on the MNIST dataset.\n",
        "You can get to 98.40% test accuracy after 20 epochs.\n",
        "'''\n",
        "\n",
        "#from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils import np_utils\n",
        "\n",
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "nb_epoch = 10\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
        "Y_test  = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(784,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# print model characteristics\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    Y_train,\n",
        "                    batch_size=batch_size, \n",
        "                    epochs=nb_epoch,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, Y_test))\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "\n",
        "print('\\n')\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "\r    8192/11490434 [..............................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.2428 - acc: 0.9254 - val_loss: 0.1165 - val_acc: 0.9641\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.1013 - acc: 0.9692 - val_loss: 0.0959 - val_acc: 0.9713\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0753 - acc: 0.9776 - val_loss: 0.0826 - val_acc: 0.9759\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0615 - acc: 0.9811 - val_loss: 0.0914 - val_acc: 0.9743\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0502 - acc: 0.9846 - val_loss: 0.0759 - val_acc: 0.9795\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0435 - acc: 0.9869 - val_loss: 0.0813 - val_acc: 0.9799\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0362 - acc: 0.9894 - val_loss: 0.0763 - val_acc: 0.9819\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.0340 - acc: 0.9901 - val_loss: 0.0828 - val_acc: 0.9836\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0312 - acc: 0.9905 - val_loss: 0.0922 - val_acc: 0.9807\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0284 - acc: 0.9919 - val_loss: 0.0864 - val_acc: 0.9831\n",
            "\n",
            "\n",
            "Test score: 0.08635157212306595\n",
            "Test accuracy: 0.9831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6m5DQRjTxhqO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ``keras`` sequential mode\n",
        "\n",
        "The ``Sequential`` model is a linear stack of layers.\n",
        "\n",
        "You can create a ``Sequential`` model by passing a list of layer instances to the constructor:\n",
        "\n",
        "```python\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(32, input_dim=784),\n",
        "    Activation('relu'),\n",
        "    Dense(10),\n",
        "    Activation('softmax'),\n",
        "])\n",
        "```\n",
        "\n",
        "You can also simply add layers via the ``.add()`` method:\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=784))\n",
        "model.add(Activation('relu'))\n",
        "...\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "29YNOobCxhqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Specifying the input shape\n",
        "\n",
        "The model needs to know what input shape it should expect. For this reason, the first layer in a ``Sequential`` model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
        "\n",
        "+ pass an ``input_shape`` argument to the first layer. This is a shape tuple (a tuple of integers or ``None`` entries, where ``None`` indicates that any positive integer may be expected). In ``input_shape``, the batch dimension is not included.\n",
        "+ pass instead a ``batch_input_shape`` argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).\n",
        "+ some 2D layers, such as ``Dense``, support the specification of their input shape via the argument ``input_dim``, and some 3D temporal layers support the arguments ``input_dim`` and ``input_length```.\n",
        "\n",
        "As such, the following three snippets are strictly equivalent:\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(784,)))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, batch_input_shape=(None, 784)))\n",
        "# note that batch dimension is \"None\" here,\n",
        "# so the model will be able to process batches of any size.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=784))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "onY5ssPAxhqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The ``merge`` layer\n",
        "\n",
        "Multiple Sequential instances can be merged into a single output via a ``Merge`` layer. The output is a layer that can be added as first layer in a new ``Sequential`` model. For instance, here's a model with two separate input branches getting merged:\n",
        "\n",
        "```python\n",
        "from keras.layers import Merge\n",
        "\n",
        "left_branch = Sequential()\n",
        "left_branch.add(Dense(32, input_dim=784))\n",
        "\n",
        "right_branch = Sequential()\n",
        "right_branch.add(Dense(32, input_dim=784))\n",
        "\n",
        "merged = Merge([left_branch, right_branch], mode='concat')\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(merged)\n",
        "final_model.add(Dense(10, activation='softmax'))\n",
        "```\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/merge.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "ShH-4C0yxhqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Merge layers: ``add``, ``subtract``, ...\n",
        "\n",
        "\n",
        "The Merge layer supports a number of pre-defined modes:\n",
        "\n",
        "sum (default): element-wise sum\n",
        "concat: tensor concatenation. You can specify the concatenation axis via the argument concat_axis.\n",
        "mul: element-wise multiplication\n",
        "ave: tensor average\n",
        "dot: dot product. You can specify which axes to reduce along via the argument dot_axes.\n",
        "cos: cosine proximity between vectors in 2D tensors.\n",
        "\n",
        "\n",
        "```python\n",
        "import keras\n",
        "\n",
        "input1 = keras.layers.Input(shape=(16,))\n",
        "x1 = keras.layers.Dense(8, activation='relu')(input1)\n",
        "input2 = keras.layers.Input(shape=(32,))\n",
        "x2 = keras.layers.Dense(8, activation='relu')(input2)\n",
        "\n",
        "added = keras.layers.Add()([x1, x2])  \n",
        "# equivalent to added = Merge([x1, x2], mode='add')\n",
        "\n",
        "\n",
        "out = keras.layers.Dense(4)(added)\n",
        "model = keras.models.Model(inputs=[input1, input2], outputs=out)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "37xsl1t1xhqR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compilation\n",
        "\n",
        "Before training a model, you need to configure the learning process, which is done via the ``compile`` method. It receives three arguments:\n",
        "\n",
        "+ an optimizer. This could be the string identifier of an existing optimizer (such as ``rmsprop`` or ``adagrad``), or an instance of the  ``Optimizer`` class. \n",
        "+ a loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as ``categorical_crossentropy`` or ``mse``), or it can be an objective function. \n",
        "+ a list of metrics. For any classification problem you will want to set this to ``metrics=['accuracy']``. A metric could be the string identifier of an existing metric or a custom metric function. Custom metric function should return either a single tensor value or a dict ``metric_name`` -> ``metric_value``. \n",
        "\n",
        "```python\n",
        "# for a multi-class classification problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# for a binary classification problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# for a mean squared error regression problem\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse')\n",
        "\n",
        "# custom metric\n",
        "import keras.backend as K\n",
        "\n",
        "def mean_pred(y_true, y_pred):\n",
        "    return K.mean(y_pred)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', mean_pred])\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "IJpe35VrxhqS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "``Keras`` models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  ``fit`` function.\n",
        "\n",
        "For a single-input model with 2 classes (binary):"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:11:41.685392Z",
          "start_time": "2019-03-05T12:11:41.142045Z"
        },
        "id": "cE73ejxtxhqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "baf14a92-8be6-471b-88c5-65748c7484a4"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim=784, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 784))\n",
        "labels = np.random.randint(2, size=(1000, 1))\n",
        "\n",
        "\n",
        "# train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, epochs=10, batch_size=32)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 140us/step - loss: 0.7207 - acc: 0.5230\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 44us/step - loss: 0.7089 - acc: 0.5260\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 37us/step - loss: 0.7072 - acc: 0.5140\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 0.6950 - acc: 0.5560\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 38us/step - loss: 0.6862 - acc: 0.5590\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 38us/step - loss: 0.6847 - acc: 0.5530\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 37us/step - loss: 0.6767 - acc: 0.5650\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 45us/step - loss: 0.6597 - acc: 0.5990\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 38us/step - loss: 0.6716 - acc: 0.5970\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 37us/step - loss: 0.6557 - acc: 0.6260\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 785       \n",
            "=================================================================\n",
            "Total params: 785\n",
            "Trainable params: 785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bndTL0mjxhqV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Keras functional API\n",
        "\n",
        "The Keras ``functional`` API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "\n",
        "The ``Sequential`` model is probably a better choice to implement such a network, but it helps to start with something really simple.\n",
        "\n",
        "Using ``Model`` class:\n",
        "\n",
        "+ A layer instance is callable (on a tensor), and it returns a tensor\n",
        "+ ``Input`` tensor(s) and output tensor(s) can then be used to define a ``Model``\n",
        "+ Such a model can be trained just like Keras Sequential models.\n",
        "\n",
        "```python\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# this returns a tensor\n",
        "inputs = Input(shape=(784,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "# this creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(input=inputs, output=predictions)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "              \n",
        "model.fit(data, labels)  # starts training\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "kbepdabCxhqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All models are callable, just like layers.\n",
        "\n",
        "With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the architecture of the model, you are also re-using its weights.\n",
        "\n",
        "```python\n",
        "x = Input(shape=(784,))\n",
        "# this works, and returns the 10-way softmax we defined above.\n",
        "y = model(x)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "Kb7I3awBxhqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Siamese MLP on pairs of digits from the MNIST \n",
        "\n",
        "(Source: https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py)\n",
        "\n",
        "Siamese networks are commonly used in image comparison applications such as face or signature verification. They can also be used in language processing, times series analysis, etc.\n",
        "\n",
        "In a typical Siamese network a large part of the network is duplicated at the base to allow multiple inputs to go through identical layers. \n",
        "\n",
        "This example shows how to teach a neural network to map an image from the MNIST dataset to a 2D point, while trying to minimize the distance between points of the same class and maximize the distance between points of different classes."
      ]
    },
    {
      "metadata": {
        "id": "lS3qKBlBxhqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Siamese network architecture is a way of learning how to embed samples into lower-dimensions based on similarity computed with features learned by a feature network.\n",
        "\n",
        "The feature network is the architecture we intend to fine-tune in this setting. \n",
        "\n",
        "Let's suppose we want to embed images. Given two images $X_1$ and $X_2$, we feed into the feature network $G_W$ and compute corresponding feature vectors $G_W(X_1)$ and $G_W(X_2)$. The final layer computes pair-wise distance between computed features $E_W = || G_W(X_1) - G_W(X_2) ||_{1}$ and final loss layer $L$ considers whether these two images are from the same class (label $1$) or not (label $0$).\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/siamese1.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "-2l1lIgexhqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the original [paper](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf) it was proposed the **Contrastive Loss Function**: \n",
        "\n",
        "$$ L(W,(Y,X_1,X_2)^i) = (1 - Y) \\times L_S(E_W(X_1,X_2)^i) + Y \\times L_D(E_W(X_1,X_2)^i) $$\n",
        "\n",
        "where $L_S$ is the partial loss function for a \"same-class\" pair and $L_D$ is the partial loss function for a \"different-class\" pair.\n",
        "\n",
        "$L_S$ and $L_D$ should be designed in such a way that the minimization of $L$ will decrease the distance in the embedding space of \"same-class\" pairs and increase it in the case of \"different-class\" pairs:\n",
        "\n",
        "$$ L_S = \\frac{1}{2} E_W^2 $$\n",
        "$$ L_D = \\frac{1}{2} \\{ \\mbox{max }(0,1-E_W) \\}^2 $$"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:22.544377Z",
          "start_time": "2019-03-05T12:11:49.850575Z"
        },
        "id": "cvzSQ-fuxhqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "e95149e7-372e-4525-f0de-5ab27d08a31d"
      },
      "cell_type": "code",
      "source": [
        "'''Train a Siamese MLP on pairs of digits from the MNIST dataset.\n",
        "It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n",
        "output of the shared network and by optimizing the contrastive loss (see paper\n",
        "for mode details).\n",
        "[1] \"Dimensionality Reduction by Learning an Invariant Mapping\"\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "Gets to 99.5% test accuracy after 20 epochs.\n",
        "3 seconds per epoch on a Titan X GPU\n",
        "'''\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf \n",
        "\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import random\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Input, Lambda\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    '''Contrastive loss from Hadsell-et-al.'06\n",
        "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    '''\n",
        "    margin = 1\n",
        "    return K.mean(y_true * K.square(y_pred) \n",
        "                  + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
        "\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    '''Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\n",
        "    '''\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
        "    for d in range(10):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, 10)\n",
        "            dn = (d + inc) % 10\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def create_base_network(input_dim):\n",
        "    '''\n",
        "    Base network to be shared (eq. to feature extraction).\n",
        "    '''\n",
        "    seq = Sequential()\n",
        "    seq.add(Dense(128, input_shape=(input_dim,), activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(128, activation='relu'))\n",
        "    seq.add(Dropout(0.1))\n",
        "    seq.add(Dense(2, activation=None,name='emb'))\n",
        "    return seq\n",
        "\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    '''Compute classification accuracy with a fixed threshold on distances.\n",
        "    '''\n",
        "    return labels[predictions.ravel() < 0.5].mean()\n",
        "\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "input_dim = 784\n",
        "nb_epoch = 10\n",
        "\n",
        "# create training+test positive and negative pairs\n",
        "digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n",
        "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
        "\n",
        "digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
        "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
        "\n",
        "# network definition\n",
        "base_network = create_base_network(input_dim)\n",
        "\n",
        "input_a = Input(shape=(input_dim,))\n",
        "input_b = Input(shape=(input_dim,))\n",
        "\n",
        "# because we re-use the same instance `base_network`,\n",
        "# the weights of the network\n",
        "# will be shared across the two branches\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "model = Model(inputs=[input_a, input_b], outputs=distance)\n",
        "\n",
        "# train\n",
        "rms = RMSprop()\n",
        "model.compile(loss=contrastive_loss, optimizer=rms)\n",
        "model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y),\n",
        "          batch_size=128,\n",
        "          epochs=nb_epoch)\n",
        "\n",
        "# compute final accuracy on training and test sets\n",
        "pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n",
        "tr_acc = compute_accuracy(pred, tr_y)\n",
        "pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n",
        "te_acc = compute_accuracy(pred, te_y)\n",
        "\n",
        "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
        "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 108400 samples, validate on 17820 samples\n",
            "Epoch 1/10\n",
            "108400/108400 [==============================] - 7s 60us/step - loss: 0.1144 - val_loss: 0.0742\n",
            "Epoch 2/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0750 - val_loss: 0.0604\n",
            "Epoch 3/10\n",
            "108400/108400 [==============================] - 6s 58us/step - loss: 0.0612 - val_loss: 0.0538\n",
            "Epoch 4/10\n",
            "108400/108400 [==============================] - 6s 58us/step - loss: 0.0538 - val_loss: 0.0493\n",
            "Epoch 5/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0481 - val_loss: 0.0460\n",
            "Epoch 6/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0441 - val_loss: 0.0461\n",
            "Epoch 7/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0411 - val_loss: 0.0447\n",
            "Epoch 8/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0385 - val_loss: 0.0423\n",
            "Epoch 9/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0363 - val_loss: 0.0415\n",
            "Epoch 10/10\n",
            "108400/108400 [==============================] - 6s 57us/step - loss: 0.0350 - val_loss: 0.0419\n",
            "* Accuracy on training set: 99.16%\n",
            "* Accuracy on test set: 97.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "668nE1igyP-Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# T-SNE visualization\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "stGZvSuRxhqc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/siameseresult.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "99Cm6C3Sxhqc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-input and multi-output models\n",
        "\n",
        "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n",
        "\n",
        "Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc. \n",
        "\n",
        "The model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n",
        "\n",
        "Here's what our model looks like:\n",
        "\n",
        "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/multi.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "XqyfvVFXxhqd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's implement it with the functional API.\n",
        "\n",
        "The main input will receive the headline, as a sequence of integers (each integer encodes a word). The integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:42.969916Z",
          "start_time": "2019-03-05T12:12:42.792604Z"
        },
        "id": "7xL_wGDGxhqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
        "# note that we can name any layer by passing it a \"name\" argument.\n",
        "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
        "\n",
        "# this embedding layer will encode the input sequence\n",
        "# into a sequence of dense 512-dimensional vectors.\n",
        "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
        "\n",
        "# a LSTM will transform the vector sequence into a single vector,\n",
        "# containing information about the entire sequence\n",
        "lstm_out = LSTM(32)(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AjqNHW7Txhqg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:44.549821Z",
          "start_time": "2019-03-05T12:12:44.536473Z"
        },
        "id": "keimozb1xhqh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vNkFLaFxhqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:46.193047Z",
          "start_time": "2019-03-05T12:12:46.146507Z"
        },
        "id": "RSWZ6wbUxhqk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
        "x = concatenate([lstm_out, auxiliary_input])\n",
        "\n",
        "# we stack a deep fully-connected network on top\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# and finally we add the main logistic regression layer\n",
        "main_output = Dense(1, activation='sigmoid', name='main_output')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ArVBVM88xhqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This defines a model with two inputs and two outputs:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:47.070464Z",
          "start_time": "2019-03-05T12:12:47.067696Z"
        },
        "id": "1_Il28Qqxhqm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0zOm38Gxhqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We compile the model and assign a weight of 0.2 to the auxiliary loss. To specify different loss_weights or loss for each different output, you can use a list or a dictionary. Here we pass a single loss as the loss argument, so the same loss will be used on all outputs."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:48.060647Z",
          "start_time": "2019-03-05T12:12:48.012672Z"
        },
        "id": "UQMhhOiUxhqq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "              loss_weights=[1., 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V0Nd3HGQxhqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can train the model by passing it lists of input arrays and target arrays:\n",
        "\n",
        "```python\n",
        "model.fit([headline_data, additional_data], [labels, labels],\n",
        "          nb_epoch=50, batch_size=32)\n",
        "```\n",
        "Since our inputs and outputs are named (we passed them a \"name\" argument), We could also have compiled the model via:\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
        "              loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
        "\n",
        "# and trained it via:\n",
        "model.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
        "          {'main_output': labels, 'aux_output': labels},\n",
        "          nb_epoch=50, batch_size=32)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "GP4F90Bbxhqu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shared layers\n",
        "\n",
        "Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n",
        "\n",
        "Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\n",
        "\n",
        "One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and adds a logistic regression of top, outputting a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\n",
        "\n",
        "Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\n",
        "\n",
        "Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape (140, 256), i.e. a sequence of 140 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters)."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:50.156057Z",
          "start_time": "2019-03-05T12:12:50.150288Z"
        },
        "id": "9RsTXcpsxhqu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense, concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "tweet_a = Input(shape=(140, 256))\n",
        "tweet_b = Input(shape=(140, 256))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C2W2wO2axhqx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-03-05T12:12:51.729970Z",
          "start_time": "2019-03-05T12:12:51.421564Z"
        },
        "id": "Nj0dQYb5xhqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this layer can take as input a matrix\n",
        "# and will return a vector of size 64\n",
        "shared_lstm = LSTM(64)\n",
        "\n",
        "# when we reuse the same layer instance\n",
        "# multiple times, the weights of the layer\n",
        "# are also being reused\n",
        "# (it is effectively *the same* layer)\n",
        "encoded_a = shared_lstm(tweet_a)\n",
        "encoded_b = shared_lstm(tweet_b)\n",
        "\n",
        "# we can then concatenate the two vectors:\n",
        "merged_vector = concatenate([encoded_a, encoded_b],axis=-1)\n",
        "\n",
        "# and add a logistic regression on top\n",
        "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
        "\n",
        "# we define a trainable model linking the\n",
        "# tweet inputs to the predictions\n",
        "model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}